{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../checkpoints/llama2/Llama-2-7b/consolidated.00.pth\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_list = []\n",
    "for key,val in ckpt.items():\n",
    "    if key.endswith(\"w1.weight\"):\n",
    "        wo_list += [val.to(\"cuda\", torch.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11008, 4096]) torch.Size([11008, 512]) torch.Size([512, 4096])\n"
     ]
    }
   ],
   "source": [
    "rank = 512\n",
    "range_anchor = 2\n",
    "w_base = torch.nn.Parameter(torch.empty_like(wo_list[0])).cuda()\n",
    "torch.nn.init.xavier_normal_(w_base)\n",
    "w_lora_list = []\n",
    "for ii in range(range_anchor):\n",
    "    lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "    lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "    torch.nn.init.xavier_normal_(lora1)\n",
    "    torch.nn.init.xavier_normal_(lora2)\n",
    "    w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "print(w_base.shape, w_lora_list[0][0].shape, w_lora_list[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 500, Loss: 1.41e-04, w:1.92e-02 grad:9.90e-11: 100%|██████████| 500/500 [00:19<00:00, 25.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=100000)\n",
    "\n",
    "pbar = tqdm([ll for ll in range(500)], desc='Training', leave=True)\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()           # Zero the gradients\n",
    "    loss = 0\n",
    "    for idx, ww in enumerate(wo_list[:range_anchor]):\n",
    "        w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "        loss += criterion(w_approx, ww)  # Calculate loss\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "    # print(w_base.grad[0,0])\n",
    "    # print(w_base[0,0])\n",
    "    optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 256\n",
    "range_anchor = 2\n",
    "# def low_rank_equivalent(base, target):\n",
    "#     delta = target.clone().detach() - base.clone().detach()\n",
    "#     u,s,v = torch.svd(delta.to(torch.float32))\n",
    "#     k = rank\n",
    "#     u_topk, s_topk, v_topk = u[:, :k], s[:k], v[:, :k]\n",
    "\n",
    "#     lora_b = torch.mm(u_topk, torch.diag(s_topk.sqrt())).to(target.dtype)\n",
    "#     lora_a = torch.mm(torch.diag(s_topk.sqrt()), v_topk.t()).to(target.dtype)\n",
    "#     return lora_a, lora_b\n",
    "# w_base = torch.nn.Parameter(wo_list[0].data.clone()).cuda()\n",
    "# w_lora_list = []\n",
    "# for ii in tqdm([kk for kk in range(1,32)]):\n",
    "#     lora_a, lora_b = low_rank_equivalent(w_base.data, wo_list[ii])\n",
    "#     lora1 = torch.nn.Parameter(lora_b.data.clone(),requires_grad=True)\n",
    "#     lora2 = torch.nn.Parameter(lora_a.data.clone(),requires_grad=True)\n",
    "#     w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "# lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "# lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "# torch.nn.init.xavier_normal_(lora1)\n",
    "# torch.nn.init.xavier_normal_(lora2)\n",
    "# w_lora_list = [(lora1.cuda(), lora2.cuda())] + w_lora_list\n",
    "\n",
    "\n",
    "w_base = torch.nn.Parameter(torch.empty_like(wo_list[0])).cuda()\n",
    "torch.nn.init.xavier_normal_(w_base)\n",
    "w_lora_list = []\n",
    "for ii in range(range_anchor):\n",
    "    lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "    lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "    torch.nn.init.xavier_normal_(lora1)\n",
    "    torch.nn.init.xavier_normal_(lora2)\n",
    "    w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "\n",
    "weight = bnb.nn.Params4bit(\n",
    "                wo_list[0].data.clone().cpu(), \n",
    "                requires_grad=False,\n",
    "                quant_type='nf4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.to(\"cuda\")\n",
    "weight_nf4 = bnb.functional.dequantize_4bit(weight, weight.quant_state)\n",
    "((wo_list[0].data.clone()-weight_nf4)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm([ll for ll in range(500)], desc='Training', leave=True)\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()           # Zero the gradients\n",
    "    loss = 0\n",
    "    for idx, ww in enumerate(wo_list[:range_anchor]):\n",
    "        w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "        loss += criterion(w_approx, ww)  # Calculate loss\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "    # print(w_base.grad[0,0])\n",
    "    # print(w_base[0,0])\n",
    "    optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_base.std(), w_base.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "(w_lora_list[idx][0] @ w_lora_list[idx][1]).std(),(w_lora_list[idx][0] @ w_lora_list[idx][1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_equivalent(base, target):\n",
    "    delta = target.clone().detach() - base.clone().detach()\n",
    "    u,s,v = torch.svd(delta.to(torch.float32))\n",
    "    k = 1024\n",
    "    u_topk, s_topk, v_topk = u[:, :k], s[:k], v[:, :k]\n",
    "\n",
    "    lora_b = torch.mm(u_topk, torch.diag(s_topk.sqrt())).to(target.dtype)\n",
    "    lora_a = torch.mm(torch.diag(s_topk.sqrt()), v_topk.t()).to(target.dtype)\n",
    "    return base + lora_b@lora_a\n",
    "delta = 0\n",
    "for ww in tqdm(wo_list[1:]):\n",
    "    new_ww = low_rank_equivalent(wo_list[0],ww)\n",
    "    delta += ((ww-new_ww)**2).mean()\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new = deepcopy(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.04e-05, w:-4.78e-03 grad:-2.79e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.0439e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.attention.wq.weight', 'layers.1.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.16e-04, w:-1.88e-03 grad:-1.39e-13: 100%|██████████| 1000/1000 [00:15<00:00, 63.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.attention.wq.weight', 'layers.3.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.38e-04, w:-1.24e-02 grad:3.34e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.attention.wq.weight', 'layers.5.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.15e-04, w:2.88e-03 grad:2.84e-13: 100%|██████████| 1000/1000 [00:15<00:00, 63.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.attention.wq.weight', 'layers.7.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.29e-04, w:-1.13e-02 grad:5.96e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.attention.wq.weight', 'layers.9.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.09e-04, w:4.73e-03 grad:6.75e-13: 100%|██████████| 1000/1000 [00:15<00:00, 63.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.attention.wq.weight', 'layers.11.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.21e-04, w:3.03e-03 grad:-1.46e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.attention.wq.weight', 'layers.13.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.15e-04, w:-1.35e-02 grad:-5.21e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.attention.wq.weight', 'layers.15.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.24e-04, w:1.60e-02 grad:1.23e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.attention.wq.weight', 'layers.17.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:-1.82e-03 grad:-3.59e-13: 100%|██████████| 1000/1000 [00:15<00:00, 63.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.attention.wq.weight', 'layers.19.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.27e-04, w:-1.09e-02 grad:-3.25e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.attention.wq.weight', 'layers.21.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.45e-04, w:8.33e-03 grad:-4.21e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.attention.wq.weight', 'layers.23.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:9.93e-03 grad:-2.29e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.attention.wq.weight', 'layers.25.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.41e-04, w:-9.19e-06 grad:2.07e-12: 100%|██████████| 1000/1000 [00:15<00:00, 63.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.attention.wq.weight', 'layers.27.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 276, Loss: 1.31e-04, w:2.33e-04 grad:-5.93e-12:  28%|██▊       | 276/1000 [00:04<00:11, 64.79it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(w_approx, ww)  \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining - Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39;49mitem()\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, w:\u001b[39m\u001b[39m{\u001b[39;00mw_base[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m grad:\u001b[39m\u001b[39m{\u001b[39;00mw_base\u001b[39m.\u001b[39mgrad[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(names):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ending_name in [\"wq.weight\",\"wk.weight\",\"wv.weight\",\"wo.weight\"]:\n",
    "    wo_list = []\n",
    "    w_name_list = []\n",
    "    for key,val in ckpt.items():\n",
    "        if key.endswith(ending_name):\n",
    "            wo_list += [val.to(\"cuda\", torch.float32)]\n",
    "            w_name_list += [key]\n",
    "\n",
    "    rank = 512\n",
    "    range_anchor = 2\n",
    "    print(f\"(In a group) full params:{4096*4096*range_anchor}, retained params:{4096*rank*2*range_anchor+4096}, reduced:{(4096*rank*2*range_anchor+4096)/(4096*4096*range_anchor):.3f}\")\n",
    "\n",
    "    for group_idx in range(0,32,range_anchor):\n",
    "        w_base = torch.nn.Parameter(torch.empty_like(wo_list[group_idx])).cuda()\n",
    "        torch.nn.init.xavier_normal_(w_base)\n",
    "        w_lora_list = []\n",
    "        for ii in range(range_anchor):\n",
    "            lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "            lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(lora1)\n",
    "            torch.nn.init.xavier_normal_(lora2)\n",
    "            w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=500000)\n",
    "\n",
    "        pbar = tqdm([ll for ll in range(1000)], desc=f'Training group_idx={group_idx}', leave=True)\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()           # Zero the gradients\n",
    "            loss = 0\n",
    "            names = w_name_list[group_idx:group_idx+range_anchor]\n",
    "            for idx, ww in enumerate(wo_list[group_idx:group_idx+range_anchor]):\n",
    "                w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "                loss += criterion(w_approx, ww)  # Calculate loss\n",
    "            loss.backward()\n",
    "            pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "            optimizer.step()\n",
    "        for layer_idx, name in enumerate(names):\n",
    "            ckpt_new[name] = w_base.to(ckpt_new[name].dtype)\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_a.weight\")] = w_lora_list[layer_idx][1]\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_b.weight\")] = w_lora_list[layer_idx][0]\n",
    "        print(group_idx,loss, names)\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt_new, \"../checkpoints/effiLLaMA2/consolidated.00.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_new[\"layers.30.attention.wq.weight\"]==ckpt_new[\"layers.31.attention.wq.weight\"])\n",
    "print(ckpt_new[\"layers.10.attention.wk.weight\"]==ckpt_new[\"layers.11.attention.wk.weight\"])\n",
    "print(ckpt_new[\"layers.2.attention.wv.weight\"]==ckpt_new[\"layers.3.attention.wv.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new[\"layers.31.attention.wq.lora_b.weight\"].shape, ckpt_new[\"layers.31.attention.wq.lora_a.weight\"].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new[\"layers.3.attention.wk.weight\"]==ckpt_new[\"layers.2.attention.wk.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llma.layers[31].attention.wq.weight\n",
    "model.llma.layers[2].attention.wk.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.06e-04, w:2.51e-02 grad:-1.46e-12: 100%|██████████| 1000/1000 [00:19<00:00, 51.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.feed_forward.w1.weight', 'layers.1.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.56e-04, w:4.45e-03 grad:-3.44e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.feed_forward.w1.weight', 'layers.3.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.74e-04, w:-6.25e-03 grad:-2.65e-13: 100%|██████████| 1000/1000 [00:19<00:00, 52.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.feed_forward.w1.weight', 'layers.5.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.73e-04, w:6.24e-03 grad:2.51e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.feed_forward.w1.weight', 'layers.7.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.63e-04, w:-6.13e-03 grad:-1.57e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.feed_forward.w1.weight', 'layers.9.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.57e-04, w:-3.85e-02 grad:1.04e-11: 100%|██████████| 1000/1000 [00:19<00:00, 52.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.feed_forward.w1.weight', 'layers.11.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.56e-04, w:1.68e-02 grad:-8.35e-15: 100%|██████████| 1000/1000 [00:19<00:00, 52.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.feed_forward.w1.weight', 'layers.13.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.58e-04, w:-1.96e-03 grad:7.29e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.feed_forward.w1.weight', 'layers.15.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.64e-04, w:-1.70e-02 grad:-5.14e-13: 100%|██████████| 1000/1000 [00:19<00:00, 52.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.feed_forward.w1.weight', 'layers.17.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.74e-04, w:-3.51e-03 grad:4.62e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.feed_forward.w1.weight', 'layers.19.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.78e-04, w:1.50e-03 grad:1.60e-11: 100%|██████████| 1000/1000 [00:19<00:00, 52.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.feed_forward.w1.weight', 'layers.21.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.83e-04, w:-4.94e-03 grad:-4.23e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.feed_forward.w1.weight', 'layers.23.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.88e-04, w:-1.99e-02 grad:6.67e-13: 100%|██████████| 1000/1000 [00:19<00:00, 52.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.feed_forward.w1.weight', 'layers.25.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.94e-04, w:2.58e-02 grad:-1.84e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.feed_forward.w1.weight', 'layers.27.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.95e-04, w:-9.67e-03 grad:1.06e-11: 100%|██████████| 1000/1000 [00:19<00:00, 52.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.28.feed_forward.w1.weight', 'layers.29.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 2.07e-04, w:-1.69e-02 grad:4.30e-12: 100%|██████████| 1000/1000 [00:19<00:00, 52.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.30.feed_forward.w1.weight', 'layers.31.feed_forward.w1.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ckpt_new = deepcopy(ckpt)\n",
    "for ending_name in [\"feed_forward.w1.weight\"]:\n",
    "    wo_list = []\n",
    "    w_name_list = []\n",
    "    for key,val in ckpt.items():\n",
    "        if key.endswith(ending_name):\n",
    "            wo_list += [val.to(\"cuda\", torch.float32)]\n",
    "            w_name_list += [key]\n",
    "\n",
    "    rank = 512\n",
    "    range_anchor = 2\n",
    "    print(f\"(In a group) full params:{4096*4096*range_anchor}, retained params:{4096*rank*2*range_anchor+4096}, reduced:{(4096*rank*2*range_anchor+4096)/(4096*4096*range_anchor):.3f}\")\n",
    "\n",
    "    for group_idx in range(0,32,range_anchor):\n",
    "        w_base = torch.nn.Parameter(torch.empty_like(wo_list[group_idx])).cuda()\n",
    "        torch.nn.init.xavier_normal_(w_base)\n",
    "        w_lora_list = []\n",
    "        for ii in range(range_anchor):\n",
    "            lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "            lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(lora1)\n",
    "            torch.nn.init.xavier_normal_(lora2)\n",
    "            w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=500000)\n",
    "\n",
    "        pbar = tqdm([ll for ll in range(1000)], desc=f'Training group_idx={group_idx}', leave=True)\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()           # Zero the gradients\n",
    "            loss = 0\n",
    "            names = w_name_list[group_idx:group_idx+range_anchor]\n",
    "            for idx, ww in enumerate(wo_list[group_idx:group_idx+range_anchor]):\n",
    "                w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "                loss += criterion(w_approx, ww)  # Calculate loss\n",
    "            loss.backward()\n",
    "            pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "            optimizer.step()\n",
    "        for layer_idx, name in enumerate(names):\n",
    "            ckpt_new[name] = w_base.to(ckpt_new[name].dtype)\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_a.weight\")] = w_lora_list[layer_idx][1]\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_b.weight\")] = w_lora_list[layer_idx][0]\n",
    "        print(group_idx,loss, names)\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accessory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
