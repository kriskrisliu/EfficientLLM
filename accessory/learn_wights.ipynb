{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../checkpoints/llama2/Llama-2-7b/consolidated.00.pth\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_list = []\n",
    "for key,val in ckpt.items():\n",
    "    if key.endswith(\"wk.weight\"):\n",
    "        wo_list += [val.to(\"cuda\", torch.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 256\n",
    "range_anchor = 2\n",
    "# def low_rank_equivalent(base, target):\n",
    "#     delta = target.clone().detach() - base.clone().detach()\n",
    "#     u,s,v = torch.svd(delta.to(torch.float32))\n",
    "#     k = rank\n",
    "#     u_topk, s_topk, v_topk = u[:, :k], s[:k], v[:, :k]\n",
    "\n",
    "#     lora_b = torch.mm(u_topk, torch.diag(s_topk.sqrt())).to(target.dtype)\n",
    "#     lora_a = torch.mm(torch.diag(s_topk.sqrt()), v_topk.t()).to(target.dtype)\n",
    "#     return lora_a, lora_b\n",
    "# w_base = torch.nn.Parameter(wo_list[0].data.clone()).cuda()\n",
    "# w_lora_list = []\n",
    "# for ii in tqdm([kk for kk in range(1,32)]):\n",
    "#     lora_a, lora_b = low_rank_equivalent(w_base.data, wo_list[ii])\n",
    "#     lora1 = torch.nn.Parameter(lora_b.data.clone(),requires_grad=True)\n",
    "#     lora2 = torch.nn.Parameter(lora_a.data.clone(),requires_grad=True)\n",
    "#     w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "# lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "# lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "# torch.nn.init.xavier_normal_(lora1)\n",
    "# torch.nn.init.xavier_normal_(lora2)\n",
    "# w_lora_list = [(lora1.cuda(), lora2.cuda())] + w_lora_list\n",
    "\n",
    "\n",
    "w_base = torch.nn.Parameter(torch.empty_like(wo_list[0])).cuda()\n",
    "torch.nn.init.xavier_normal_(w_base)\n",
    "w_lora_list = []\n",
    "for ii in range(range_anchor):\n",
    "    lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "    lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "    torch.nn.init.xavier_normal_(lora1)\n",
    "    torch.nn.init.xavier_normal_(lora2)\n",
    "    w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "\n",
    "weight = bnb.nn.Params4bit(\n",
    "                wo_list[0].data.clone().cpu(), \n",
    "                requires_grad=False,\n",
    "                quant_type='nf4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2611e-06, device='cuda:0')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.to(\"cuda\")\n",
    "weight_nf4 = bnb.functional.dequantize_4bit(weight, weight.quant_state)\n",
    "((wo_list[0].data.clone()-weight_nf4)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 500, Loss: 3.86e-05, w:2.99e-03 grad:-6.86e-12: 100%|██████████| 500/500 [00:02<00:00, 216.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8600e-05, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm([ll for ll in range(500)], desc='Training', leave=True)\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()           # Zero the gradients\n",
    "    loss = 0\n",
    "    for idx, ww in enumerate(wo_list[:range_anchor]):\n",
    "        w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "        loss += criterion(w_approx, ww)  # Calculate loss\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "    # print(w_base.grad[0,0])\n",
    "    # print(w_base[0,0])\n",
    "    optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0144, device='cuda:0', grad_fn=<StdBackward0>),\n",
       " tensor(3.7319e-06, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_base.std(), w_base.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0174, device='cuda:0', grad_fn=<StdBackward0>),\n",
       " tensor(3.2446e-06, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=0\n",
    "(w_lora_list[idx][0] @ w_lora_list[idx][1]).std(),(w_lora_list[idx][0] @ w_lora_list[idx][1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:59<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0039, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def low_rank_equivalent(base, target):\n",
    "    delta = target.clone().detach() - base.clone().detach()\n",
    "    u,s,v = torch.svd(delta.to(torch.float32))\n",
    "    k = 1024\n",
    "    u_topk, s_topk, v_topk = u[:, :k], s[:k], v[:, :k]\n",
    "\n",
    "    lora_b = torch.mm(u_topk, torch.diag(s_topk.sqrt())).to(target.dtype)\n",
    "    lora_a = torch.mm(torch.diag(s_topk.sqrt()), v_topk.t()).to(target.dtype)\n",
    "    return base + lora_b@lora_a\n",
    "delta = 0\n",
    "for ww in tqdm(wo_list[1:]):\n",
    "    new_ww = low_rank_equivalent(wo_list[0],ww)\n",
    "    delta += ((ww-new_ww)**2).mean()\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new = deepcopy(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.04e-05, w:4.93e-03 grad:9.92e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.0443e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.attention.wq.weight', 'layers.1.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.16e-04, w:-8.26e-03 grad:-1.39e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.attention.wq.weight', 'layers.3.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.38e-04, w:-1.85e-02 grad:3.17e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.attention.wq.weight', 'layers.5.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.15e-04, w:-1.38e-02 grad:-1.72e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.attention.wq.weight', 'layers.7.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.29e-04, w:1.74e-03 grad:-1.64e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.attention.wq.weight', 'layers.9.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.09e-04, w:4.75e-03 grad:1.91e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.attention.wq.weight', 'layers.11.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.21e-04, w:-7.91e-03 grad:1.58e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.attention.wq.weight', 'layers.13.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.15e-04, w:1.15e-02 grad:-2.86e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.attention.wq.weight', 'layers.15.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.24e-04, w:-3.82e-03 grad:-4.10e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.attention.wq.weight', 'layers.17.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:-9.23e-03 grad:2.44e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.attention.wq.weight', 'layers.19.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.27e-04, w:-4.46e-03 grad:2.90e-13: 100%|██████████| 1000/1000 [00:07<00:00, 130.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.attention.wq.weight', 'layers.21.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.45e-04, w:-6.03e-03 grad:-3.72e-13: 100%|██████████| 1000/1000 [00:07<00:00, 130.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.attention.wq.weight', 'layers.23.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:-4.72e-03 grad:3.03e-12: 100%|██████████| 1000/1000 [00:07<00:00, 129.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.attention.wq.weight', 'layers.25.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.41e-04, w:-1.61e-02 grad:2.50e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.attention.wq.weight', 'layers.27.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.29e-04, w:5.77e-03 grad:-2.91e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.28.attention.wq.weight', 'layers.29.attention.wq.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.22e-04, w:-2.95e-02 grad:1.58e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.30.attention.wq.weight', 'layers.31.attention.wq.weight']\n",
      "----------------------------------------\n",
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 9.09e-06, w:1.01e-02 grad:-1.23e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(9.0937e-06, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.attention.wk.weight', 'layers.1.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.26e-04, w:-8.82e-04 grad:-2.61e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.attention.wk.weight', 'layers.3.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.51e-04, w:-2.40e-02 grad:1.43e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.attention.wk.weight', 'layers.5.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.24e-04, w:-1.77e-03 grad:-5.86e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.attention.wk.weight', 'layers.7.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.41e-04, w:-2.54e-02 grad:-3.01e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.attention.wk.weight', 'layers.9.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.20e-04, w:-1.81e-02 grad:2.55e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.attention.wk.weight', 'layers.11.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:1.43e-02 grad:-3.00e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.attention.wk.weight', 'layers.13.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.24e-04, w:-2.21e-03 grad:-5.59e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.attention.wk.weight', 'layers.15.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.31e-04, w:-1.16e-02 grad:-3.38e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.attention.wk.weight', 'layers.17.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.40e-04, w:6.06e-03 grad:3.50e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.attention.wk.weight', 'layers.19.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:-1.02e-03 grad:-2.07e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.attention.wk.weight', 'layers.21.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.50e-04, w:-1.34e-02 grad:4.90e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.attention.wk.weight', 'layers.23.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.37e-04, w:2.62e-03 grad:-3.48e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.attention.wk.weight', 'layers.25.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.48e-04, w:1.34e-04 grad:-1.95e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.attention.wk.weight', 'layers.27.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.37e-04, w:1.22e-02 grad:1.28e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.28.attention.wk.weight', 'layers.29.attention.wk.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.32e-04, w:-4.70e-03 grad:-4.29e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.30.attention.wk.weight', 'layers.31.attention.wk.weight']\n",
      "----------------------------------------\n",
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 2.27e-05, w:3.96e-03 grad:-2.75e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.2744e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.attention.wv.weight', 'layers.1.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.94e-05, w:3.50e-03 grad:-1.35e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(5.9404e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.attention.wv.weight', 'layers.3.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.69e-05, w:8.34e-04 grad:3.10e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(6.6884e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.attention.wv.weight', 'layers.5.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.64e-05, w:4.73e-03 grad:-1.34e-11: 100%|██████████| 1000/1000 [00:07<00:00, 131.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(5.6397e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.attention.wv.weight', 'layers.7.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.78e-05, w:-4.66e-03 grad:3.75e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(5.7849e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.attention.wv.weight', 'layers.9.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.67e-05, w:-1.94e-02 grad:4.15e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(5.6732e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.attention.wv.weight', 'layers.11.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.58e-05, w:1.22e-02 grad:2.06e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(6.5776e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.attention.wv.weight', 'layers.13.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.97e-05, w:1.10e-02 grad:-1.18e-11: 100%|██████████| 1000/1000 [00:07<00:00, 130.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(6.9748e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.attention.wv.weight', 'layers.15.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 8.82e-05, w:-1.18e-02 grad:8.42e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(8.8180e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.attention.wv.weight', 'layers.17.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 9.88e-05, w:-3.06e-04 grad:-4.10e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(9.8769e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.attention.wv.weight', 'layers.19.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.06e-04, w:3.66e-03 grad:4.65e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.attention.wv.weight', 'layers.21.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.19e-04, w:-9.93e-03 grad:7.47e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.attention.wv.weight', 'layers.23.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.25e-04, w:2.48e-02 grad:2.28e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.attention.wv.weight', 'layers.25.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.39e-04, w:4.02e-02 grad:3.99e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.attention.wv.weight', 'layers.27.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.49e-04, w:1.10e-02 grad:-3.39e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.28.attention.wv.weight', 'layers.29.attention.wv.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.45e-04, w:7.61e-03 grad:2.12e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.30.attention.wv.weight', 'layers.31.attention.wv.weight']\n",
      "----------------------------------------\n",
      "(In a group) full params:33554432, retained params:8392704, reduced:0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.25e-05, w:1.03e-02 grad:-2.80e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.2460e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.0.attention.wo.weight', 'layers.1.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.92e-05, w:-1.59e-02 grad:-3.98e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(5.9250e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.2.attention.wo.weight', 'layers.3.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.30e-05, w:1.66e-02 grad:-2.58e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(6.3013e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.4.attention.wo.weight', 'layers.5.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.35e-05, w:1.22e-02 grad:-5.86e-12: 100%|██████████| 1000/1000 [00:07<00:00, 132.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(5.3504e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.6.attention.wo.weight', 'layers.7.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.78e-05, w:2.69e-02 grad:-3.33e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(5.7828e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.8.attention.wo.weight', 'layers.9.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 5.42e-05, w:-4.19e-03 grad:9.08e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(5.4223e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.10.attention.wo.weight', 'layers.11.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.15e-05, w:7.09e-04 grad:-6.45e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(6.1539e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.12.attention.wo.weight', 'layers.13.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 6.59e-05, w:-3.95e-03 grad:-5.63e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(6.5853e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.14.attention.wo.weight', 'layers.15.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 8.56e-05, w:9.19e-03 grad:-1.83e-12: 100%|██████████| 1000/1000 [00:07<00:00, 130.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(8.5564e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.16.attention.wo.weight', 'layers.17.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 9.94e-05, w:-5.30e-03 grad:7.77e-14: 100%|██████████| 1000/1000 [00:07<00:00, 131.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(9.9391e-05, device='cuda:0', grad_fn=<AddBackward0>) ['layers.18.attention.wo.weight', 'layers.19.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.13e-04, w:-2.27e-02 grad:7.68e-13: 100%|██████████| 1000/1000 [00:07<00:00, 131.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.20.attention.wo.weight', 'layers.21.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.21e-04, w:-1.73e-02 grad:-1.95e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.22.attention.wo.weight', 'layers.23.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.27e-04, w:-8.90e-03 grad:-8.30e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.24.attention.wo.weight', 'layers.25.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.38e-04, w:1.25e-02 grad:-6.57e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.26.attention.wo.weight', 'layers.27.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.47e-04, w:1.18e-02 grad:-5.97e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.28.attention.wo.weight', 'layers.29.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1000, Loss: 1.43e-04, w:1.14e-02 grad:-2.65e-12: 100%|██████████| 1000/1000 [00:07<00:00, 131.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>) ['layers.30.attention.wo.weight', 'layers.31.attention.wo.weight']\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ending_name in [\"wq.weight\",\"wk.weight\",\"wv.weight\",\"wo.weight\"]:\n",
    "    wo_list = []\n",
    "    w_name_list = []\n",
    "    for key,val in ckpt.items():\n",
    "        if key.endswith(ending_name):\n",
    "            wo_list += [val.to(\"cuda\", torch.float32)]\n",
    "            w_name_list += [key]\n",
    "\n",
    "    rank = 512\n",
    "    range_anchor = 2\n",
    "    print(f\"(In a group) full params:{4096*4096*range_anchor}, retained params:{4096*rank*2*range_anchor+4096}, reduced:{(4096*rank*2*range_anchor+4096)/(4096*4096*range_anchor):.3f}\")\n",
    "\n",
    "    for group_idx in range(0,32,range_anchor):\n",
    "        w_base = torch.nn.Parameter(torch.empty_like(wo_list[group_idx])).cuda()\n",
    "        torch.nn.init.xavier_normal_(w_base)\n",
    "        w_lora_list = []\n",
    "        for ii in range(range_anchor):\n",
    "            lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "            lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(lora1)\n",
    "            torch.nn.init.xavier_normal_(lora2)\n",
    "            w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=500000)\n",
    "\n",
    "        pbar = tqdm([ll for ll in range(1000)], desc=f'Training group_idx={group_idx}', leave=True)\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()           # Zero the gradients\n",
    "            loss = 0\n",
    "            names = w_name_list[group_idx:group_idx+range_anchor]\n",
    "            for idx, ww in enumerate(wo_list[group_idx:group_idx+range_anchor]):\n",
    "                w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "                loss += criterion(w_approx, ww)  # Calculate loss\n",
    "            loss.backward()\n",
    "            pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "            optimizer.step()\n",
    "        for layer_idx, name in enumerate(names):\n",
    "            ckpt_new[name] = w_base.to(ckpt_new[name].dtype)\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_a.weight\")] = w_lora_list[layer_idx][1]\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_b.weight\")] = w_lora_list[layer_idx][0]\n",
    "        print(group_idx,loss, names)\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'norm.weight', 'output.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wq.weight', 'layers.28.attention.wk.weight', 'layers.28.attention.wv.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.attention_norm.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wq.weight', 'layers.29.attention.wk.weight', 'layers.29.attention.wv.weight', 'layers.29.attention.wo.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.attention_norm.weight', 'layers.29.ffn_norm.weight', 'layers.30.attention.wq.weight', 'layers.30.attention.wk.weight', 'layers.30.attention.wv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.attention_norm.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.31.attention.wk.weight', 'layers.31.attention.wv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.attention_norm.weight', 'layers.31.ffn_norm.weight', 'rope.freqs', 'layers.0.attention.wq.lora_a.weight', 'layers.0.attention.wq.lora_b.weight', 'layers.1.attention.wq.lora_a.weight', 'layers.1.attention.wq.lora_b.weight', 'layers.2.attention.wq.lora_a.weight', 'layers.2.attention.wq.lora_b.weight', 'layers.3.attention.wq.lora_a.weight', 'layers.3.attention.wq.lora_b.weight', 'layers.4.attention.wq.lora_a.weight', 'layers.4.attention.wq.lora_b.weight', 'layers.5.attention.wq.lora_a.weight', 'layers.5.attention.wq.lora_b.weight', 'layers.6.attention.wq.lora_a.weight', 'layers.6.attention.wq.lora_b.weight', 'layers.7.attention.wq.lora_a.weight', 'layers.7.attention.wq.lora_b.weight', 'layers.8.attention.wq.lora_a.weight', 'layers.8.attention.wq.lora_b.weight', 'layers.9.attention.wq.lora_a.weight', 'layers.9.attention.wq.lora_b.weight', 'layers.10.attention.wq.lora_a.weight', 'layers.10.attention.wq.lora_b.weight', 'layers.11.attention.wq.lora_a.weight', 'layers.11.attention.wq.lora_b.weight', 'layers.12.attention.wq.lora_a.weight', 'layers.12.attention.wq.lora_b.weight', 'layers.13.attention.wq.lora_a.weight', 'layers.13.attention.wq.lora_b.weight', 'layers.14.attention.wq.lora_a.weight', 'layers.14.attention.wq.lora_b.weight', 'layers.15.attention.wq.lora_a.weight', 'layers.15.attention.wq.lora_b.weight', 'layers.16.attention.wq.lora_a.weight', 'layers.16.attention.wq.lora_b.weight', 'layers.17.attention.wq.lora_a.weight', 'layers.17.attention.wq.lora_b.weight', 'layers.18.attention.wq.lora_a.weight', 'layers.18.attention.wq.lora_b.weight', 'layers.19.attention.wq.lora_a.weight', 'layers.19.attention.wq.lora_b.weight', 'layers.20.attention.wq.lora_a.weight', 'layers.20.attention.wq.lora_b.weight', 'layers.21.attention.wq.lora_a.weight', 'layers.21.attention.wq.lora_b.weight', 'layers.22.attention.wq.lora_a.weight', 'layers.22.attention.wq.lora_b.weight', 'layers.23.attention.wq.lora_a.weight', 'layers.23.attention.wq.lora_b.weight', 'layers.24.attention.wq.lora_a.weight', 'layers.24.attention.wq.lora_b.weight', 'layers.25.attention.wq.lora_a.weight', 'layers.25.attention.wq.lora_b.weight', 'layers.26.attention.wq.lora_a.weight', 'layers.26.attention.wq.lora_b.weight', 'layers.27.attention.wq.lora_a.weight', 'layers.27.attention.wq.lora_b.weight', 'layers.28.attention.wq.lora_a.weight', 'layers.28.attention.wq.lora_b.weight', 'layers.29.attention.wq.lora_a.weight', 'layers.29.attention.wq.lora_b.weight', 'layers.30.attention.wq.lora_a.weight', 'layers.30.attention.wq.lora_b.weight', 'layers.31.attention.wq.lora_a.weight', 'layers.31.attention.wq.lora_b.weight', 'layers.0.attention.wk.lora_a.weight', 'layers.0.attention.wk.lora_b.weight', 'layers.1.attention.wk.lora_a.weight', 'layers.1.attention.wk.lora_b.weight', 'layers.2.attention.wk.lora_a.weight', 'layers.2.attention.wk.lora_b.weight', 'layers.3.attention.wk.lora_a.weight', 'layers.3.attention.wk.lora_b.weight', 'layers.4.attention.wk.lora_a.weight', 'layers.4.attention.wk.lora_b.weight', 'layers.5.attention.wk.lora_a.weight', 'layers.5.attention.wk.lora_b.weight', 'layers.6.attention.wk.lora_a.weight', 'layers.6.attention.wk.lora_b.weight', 'layers.7.attention.wk.lora_a.weight', 'layers.7.attention.wk.lora_b.weight', 'layers.8.attention.wk.lora_a.weight', 'layers.8.attention.wk.lora_b.weight', 'layers.9.attention.wk.lora_a.weight', 'layers.9.attention.wk.lora_b.weight', 'layers.10.attention.wk.lora_a.weight', 'layers.10.attention.wk.lora_b.weight', 'layers.11.attention.wk.lora_a.weight', 'layers.11.attention.wk.lora_b.weight', 'layers.12.attention.wk.lora_a.weight', 'layers.12.attention.wk.lora_b.weight', 'layers.13.attention.wk.lora_a.weight', 'layers.13.attention.wk.lora_b.weight', 'layers.14.attention.wk.lora_a.weight', 'layers.14.attention.wk.lora_b.weight', 'layers.15.attention.wk.lora_a.weight', 'layers.15.attention.wk.lora_b.weight', 'layers.16.attention.wk.lora_a.weight', 'layers.16.attention.wk.lora_b.weight', 'layers.17.attention.wk.lora_a.weight', 'layers.17.attention.wk.lora_b.weight', 'layers.18.attention.wk.lora_a.weight', 'layers.18.attention.wk.lora_b.weight', 'layers.19.attention.wk.lora_a.weight', 'layers.19.attention.wk.lora_b.weight', 'layers.20.attention.wk.lora_a.weight', 'layers.20.attention.wk.lora_b.weight', 'layers.21.attention.wk.lora_a.weight', 'layers.21.attention.wk.lora_b.weight', 'layers.22.attention.wk.lora_a.weight', 'layers.22.attention.wk.lora_b.weight', 'layers.23.attention.wk.lora_a.weight', 'layers.23.attention.wk.lora_b.weight', 'layers.24.attention.wk.lora_a.weight', 'layers.24.attention.wk.lora_b.weight', 'layers.25.attention.wk.lora_a.weight', 'layers.25.attention.wk.lora_b.weight', 'layers.26.attention.wk.lora_a.weight', 'layers.26.attention.wk.lora_b.weight', 'layers.27.attention.wk.lora_a.weight', 'layers.27.attention.wk.lora_b.weight', 'layers.28.attention.wk.lora_a.weight', 'layers.28.attention.wk.lora_b.weight', 'layers.29.attention.wk.lora_a.weight', 'layers.29.attention.wk.lora_b.weight', 'layers.30.attention.wk.lora_a.weight', 'layers.30.attention.wk.lora_b.weight', 'layers.31.attention.wk.lora_a.weight', 'layers.31.attention.wk.lora_b.weight', 'layers.0.attention.wv.lora_a.weight', 'layers.0.attention.wv.lora_b.weight', 'layers.1.attention.wv.lora_a.weight', 'layers.1.attention.wv.lora_b.weight', 'layers.2.attention.wv.lora_a.weight', 'layers.2.attention.wv.lora_b.weight', 'layers.3.attention.wv.lora_a.weight', 'layers.3.attention.wv.lora_b.weight', 'layers.4.attention.wv.lora_a.weight', 'layers.4.attention.wv.lora_b.weight', 'layers.5.attention.wv.lora_a.weight', 'layers.5.attention.wv.lora_b.weight', 'layers.6.attention.wv.lora_a.weight', 'layers.6.attention.wv.lora_b.weight', 'layers.7.attention.wv.lora_a.weight', 'layers.7.attention.wv.lora_b.weight', 'layers.8.attention.wv.lora_a.weight', 'layers.8.attention.wv.lora_b.weight', 'layers.9.attention.wv.lora_a.weight', 'layers.9.attention.wv.lora_b.weight', 'layers.10.attention.wv.lora_a.weight', 'layers.10.attention.wv.lora_b.weight', 'layers.11.attention.wv.lora_a.weight', 'layers.11.attention.wv.lora_b.weight', 'layers.12.attention.wv.lora_a.weight', 'layers.12.attention.wv.lora_b.weight', 'layers.13.attention.wv.lora_a.weight', 'layers.13.attention.wv.lora_b.weight', 'layers.14.attention.wv.lora_a.weight', 'layers.14.attention.wv.lora_b.weight', 'layers.15.attention.wv.lora_a.weight', 'layers.15.attention.wv.lora_b.weight', 'layers.16.attention.wv.lora_a.weight', 'layers.16.attention.wv.lora_b.weight', 'layers.17.attention.wv.lora_a.weight', 'layers.17.attention.wv.lora_b.weight', 'layers.18.attention.wv.lora_a.weight', 'layers.18.attention.wv.lora_b.weight', 'layers.19.attention.wv.lora_a.weight', 'layers.19.attention.wv.lora_b.weight', 'layers.20.attention.wv.lora_a.weight', 'layers.20.attention.wv.lora_b.weight', 'layers.21.attention.wv.lora_a.weight', 'layers.21.attention.wv.lora_b.weight', 'layers.22.attention.wv.lora_a.weight', 'layers.22.attention.wv.lora_b.weight', 'layers.23.attention.wv.lora_a.weight', 'layers.23.attention.wv.lora_b.weight', 'layers.24.attention.wv.lora_a.weight', 'layers.24.attention.wv.lora_b.weight', 'layers.25.attention.wv.lora_a.weight', 'layers.25.attention.wv.lora_b.weight', 'layers.26.attention.wv.lora_a.weight', 'layers.26.attention.wv.lora_b.weight', 'layers.27.attention.wv.lora_a.weight', 'layers.27.attention.wv.lora_b.weight', 'layers.28.attention.wv.lora_a.weight', 'layers.28.attention.wv.lora_b.weight', 'layers.29.attention.wv.lora_a.weight', 'layers.29.attention.wv.lora_b.weight', 'layers.30.attention.wv.lora_a.weight', 'layers.30.attention.wv.lora_b.weight', 'layers.31.attention.wv.lora_a.weight', 'layers.31.attention.wv.lora_b.weight', 'layers.0.attention.wo.lora_a.weight', 'layers.0.attention.wo.lora_b.weight', 'layers.1.attention.wo.lora_a.weight', 'layers.1.attention.wo.lora_b.weight', 'layers.2.attention.wo.lora_a.weight', 'layers.2.attention.wo.lora_b.weight', 'layers.3.attention.wo.lora_a.weight', 'layers.3.attention.wo.lora_b.weight', 'layers.4.attention.wo.lora_a.weight', 'layers.4.attention.wo.lora_b.weight', 'layers.5.attention.wo.lora_a.weight', 'layers.5.attention.wo.lora_b.weight', 'layers.6.attention.wo.lora_a.weight', 'layers.6.attention.wo.lora_b.weight', 'layers.7.attention.wo.lora_a.weight', 'layers.7.attention.wo.lora_b.weight', 'layers.8.attention.wo.lora_a.weight', 'layers.8.attention.wo.lora_b.weight', 'layers.9.attention.wo.lora_a.weight', 'layers.9.attention.wo.lora_b.weight', 'layers.10.attention.wo.lora_a.weight', 'layers.10.attention.wo.lora_b.weight', 'layers.11.attention.wo.lora_a.weight', 'layers.11.attention.wo.lora_b.weight', 'layers.12.attention.wo.lora_a.weight', 'layers.12.attention.wo.lora_b.weight', 'layers.13.attention.wo.lora_a.weight', 'layers.13.attention.wo.lora_b.weight', 'layers.14.attention.wo.lora_a.weight', 'layers.14.attention.wo.lora_b.weight', 'layers.15.attention.wo.lora_a.weight', 'layers.15.attention.wo.lora_b.weight', 'layers.16.attention.wo.lora_a.weight', 'layers.16.attention.wo.lora_b.weight', 'layers.17.attention.wo.lora_a.weight', 'layers.17.attention.wo.lora_b.weight', 'layers.18.attention.wo.lora_a.weight', 'layers.18.attention.wo.lora_b.weight', 'layers.19.attention.wo.lora_a.weight', 'layers.19.attention.wo.lora_b.weight', 'layers.20.attention.wo.lora_a.weight', 'layers.20.attention.wo.lora_b.weight', 'layers.21.attention.wo.lora_a.weight', 'layers.21.attention.wo.lora_b.weight', 'layers.22.attention.wo.lora_a.weight', 'layers.22.attention.wo.lora_b.weight', 'layers.23.attention.wo.lora_a.weight', 'layers.23.attention.wo.lora_b.weight', 'layers.24.attention.wo.lora_a.weight', 'layers.24.attention.wo.lora_b.weight', 'layers.25.attention.wo.lora_a.weight', 'layers.25.attention.wo.lora_b.weight', 'layers.26.attention.wo.lora_a.weight', 'layers.26.attention.wo.lora_b.weight', 'layers.27.attention.wo.lora_a.weight', 'layers.27.attention.wo.lora_b.weight', 'layers.28.attention.wo.lora_a.weight', 'layers.28.attention.wo.lora_b.weight', 'layers.29.attention.wo.lora_a.weight', 'layers.29.attention.wo.lora_b.weight', 'layers.30.attention.wo.lora_a.weight', 'layers.30.attention.wo.lora_b.weight', 'layers.31.attention.wo.lora_a.weight', 'layers.31.attention.wo.lora_b.weight'])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt_new, \"../checkpoints/effiLLaMA2/consolidated.00.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]], device='cuda:0')\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]], device='cuda:0')\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ckpt_new[\"layers.30.attention.wq.weight\"]==ckpt_new[\"layers.31.attention.wq.weight\"])\n",
    "print(ckpt_new[\"layers.10.attention.wk.weight\"]==ckpt_new[\"layers.11.attention.wk.weight\"])\n",
    "print(ckpt_new[\"layers.2.attention.wv.weight\"]==ckpt_new[\"layers.3.attention.wv.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 512]), torch.Size([512, 4096]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_new[\"layers.31.attention.wq.lora_b.weight\"].shape, ckpt_new[\"layers.31.attention.wq.lora_a.weight\"].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_new[\"layers.3.attention.wk.weight\"]==ckpt_new[\"layers.2.attention.wk.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llma.layers[31].attention.wq.weight\n",
    "model.llma.layers[2].attention.wk.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ending_name in [\"feed_forward.w1\"]:\n",
    "    wo_list = []\n",
    "    w_name_list = []\n",
    "    for key,val in ckpt.items():\n",
    "        if key.endswith(ending_name):\n",
    "            wo_list += [val.to(\"cuda\", torch.float32)]\n",
    "            w_name_list += [key]\n",
    "\n",
    "    rank = 512\n",
    "    range_anchor = 2\n",
    "    print(f\"(In a group) full params:{4096*4096*range_anchor}, retained params:{4096*rank*2*range_anchor+4096}, reduced:{(4096*rank*2*range_anchor+4096)/(4096*4096*range_anchor):.3f}\")\n",
    "\n",
    "    for group_idx in range(0,32,range_anchor):\n",
    "        w_base = torch.nn.Parameter(torch.empty_like(wo_list[group_idx])).cuda()\n",
    "        torch.nn.init.xavier_normal_(w_base)\n",
    "        w_lora_list = []\n",
    "        for ii in range(range_anchor):\n",
    "            lora1 = torch.nn.Parameter(torch.empty_like(wo_list[0][:,:rank]),requires_grad=True)\n",
    "            lora2 = torch.nn.Parameter(torch.empty_like(wo_list[0][:rank,:]),requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(lora1)\n",
    "            torch.nn.init.xavier_normal_(lora2)\n",
    "            w_lora_list +=[(lora1.cuda(), lora2.cuda())]\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD([w_lora_list[ii][0] for ii in range(range_anchor)]+[w_lora_list[ii][1] for ii in range(range_anchor)]+[w_base], lr=500000)\n",
    "\n",
    "        pbar = tqdm([ll for ll in range(1000)], desc=f'Training group_idx={group_idx}', leave=True)\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()           # Zero the gradients\n",
    "            loss = 0\n",
    "            names = w_name_list[group_idx:group_idx+range_anchor]\n",
    "            for idx, ww in enumerate(wo_list[group_idx:group_idx+range_anchor]):\n",
    "                w_approx = w_base + w_lora_list[idx][0] @ w_lora_list[idx][1]\n",
    "                loss += criterion(w_approx, ww)  # Calculate loss\n",
    "            loss.backward()\n",
    "            pbar.set_description(f'Training - Epoch {epoch+1}, Loss: {loss.item():.2e}, w:{w_base[0,0].item():.2e} grad:{w_base.grad[0,0].item():.2e}')\n",
    "            optimizer.step()\n",
    "        for layer_idx, name in enumerate(names):\n",
    "            ckpt_new[name] = w_base.to(ckpt_new[name].dtype)\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_a.weight\")] = w_lora_list[layer_idx][1]\n",
    "            ckpt_new[name.replace(\"weight\",\"lora_b.weight\")] = w_lora_list[layer_idx][0]\n",
    "        print(group_idx,loss, names)\n",
    "        print(\"-\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accessory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
